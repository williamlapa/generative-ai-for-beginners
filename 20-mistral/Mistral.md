# Construindo com Modelos Mistral

## Introdu√ß√£o

Esta li√ß√£o abordar√°:

* Explora√ß√£o dos diferentes modelos Mistral.
* Compreens√£o dos casos de uso e cen√°rios para cada modelo.
* Exemplos de c√≥digo mostrando os recursos exclusivos de cada modelo.

## Os Modelos Mistral

Nesta li√ß√£o, exploraremos tr√™s modelos Mistral diferentes:

 **Mistral Large** , **Mistral Small** e  **Mistral NeMo** .

Cada um desses modelos est√° dispon√≠vel gratuitamente no marketplace de modelos do GitHub. O c√≥digo neste notebook utilizar√° esses modelos para demonstra√ß√£o. Aqui est√£o mais detalhes sobre como usar o GitHub Models para [prototipar com modelos de IA](https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst).

---

## **Mistral Large 2 (2407)**

O **Mistral Large 2** √© atualmente o modelo principal da Mistral, projetado para uso corporativo.

Este modelo √© uma melhoria do Mistral Large original, oferecendo:

* **Janela de contexto maior** ‚Üí 128k tokens vs 32k tokens.
* **Melhor desempenho em tarefas matem√°ticas e de programa√ß√£o** ‚Üí 76,9% de precis√£o m√©dia vs 60,4%.
* **Suporte multil√≠ngue aprimorado** ‚Üí Idiomas suportados: Ingl√™s, Franc√™s, Alem√£o, Espanhol, Italiano, Portugu√™s, Holand√™s, Russo, Chin√™s, Japon√™s, Coreano, √Årabe e Hindi.

Com esses recursos, o **Mistral Large 2** se destaca em:

* **Gera√ß√£o aumentada por recupera√ß√£o (RAG)** ‚Üí Gra√ßas √† sua grande janela de contexto.
* **Chamada de fun√ß√µes (Function Calling)** ‚Üí Integra√ß√£o nativa com ferramentas e APIs externas, permitindo chamadas paralelas ou sequenciais.
* **Gera√ß√£o de c√≥digo** ‚Üí Excelente desempenho na gera√ß√£o de c√≥digo em  **Python, Java, TypeScript e C++** .

### Exemplo de RAG usando o Mistral Large 2

Este exemplo usa o Mistral Large 2 para aplicar um padr√£o de **RAG (Retrieval-Augmented Generation)** sobre um documento de texto. A pergunta est√° em coreano e questiona as atividades do autor antes da faculdade.

O c√≥digo utiliza o **modelo de embeddings da Cohere** para criar embeddings tanto do documento quanto da pergunta. O armazenamento vetorial √© feito usando  **faiss** . O prompt enviado ao modelo Mistral inclui a pergunta e os trechos recuperados mais relevantes. O modelo ent√£o gera uma resposta em linguagem natural.

```python
pip install faiss-cpu
```


```python
import requests
import numpy as np
import faiss
import os

from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import SystemMessage, UserMessage
from azure.core.credentials import AzureKeyCredential
from azure.ai.inference import EmbeddingsClient

endpoint = "https://models.inference.ai.azure.com"
model_name = "Mistral-large"
token = os.environ["GITHUB_TOKEN"]

client = ChatCompletionsClient(
    endpoint=endpoint,
    credential=AzureKeyCredential(token),
)

response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')
text = response.text

chunk_size = 2048
chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
len(chunks)

embed_model_name = "cohere-embed-v3-multilingual" 

embed_client = EmbeddingsClient(
        endpoint=endpoint,
        credential=AzureKeyCredential(token)
)

embed_response = embed_client.embed(
    input=chunks,
    model=embed_model_name
)



text_embeddings = []
for item in embed_response.data:
    length = len(item.embedding)
    text_embeddings.append(item.embedding)
text_embeddings = np.array(text_embeddings)


d = text_embeddings.shape[1]
index = faiss.IndexFlatL2(d)
index.add(text_embeddings)

question = "Ï†ÄÏûêÍ∞Ä ÎåÄÌïôÏóê Ïò§Í∏∞ Ï†ÑÏóê Ï£ºÎ°ú ÌñàÎçò Îëê Í∞ÄÏßÄ ÏùºÏùÄ Î¨¥ÏóáÏù¥ÏóàÎÇòÏöî?Ôºü"

question_embedding = embed_client.embed(
    input=[question],
    model=embed_model_name
)

question_embeddings = np.array(question_embedding.data[0].embedding)


D, I = index.search(question_embeddings.reshape(1, -1), k=2) # distance, index
retrieved_chunks = [chunks[i] for i in I.tolist()[0]]

prompt = f"""
Context information is below.
---------------------
{retrieved_chunks}
---------------------
Given the context information and not prior knowledge, answer the query.
Query: {question}
Answer:
"""


chat_response = client.complete(
    messages=[
        SystemMessage(content="You are a helpful assistant."),
        UserMessage(content=prompt),
    ],
    temperature=1.0,
    top_p=1.0,
    max_tokens=1000,
    model=model_name
)

print(chat_response.choices[0].message.content)import
```

(O c√≥digo continua...)

---

## **Mistral Small**

O **Mistral Small** √© outro modelo da fam√≠lia Mistral, classificado como um  **Small Language Model (SLM)** .

### **Vantagens do Mistral Small** :

* **Mais econ√¥mico** ‚Üí 80% de redu√ß√£o de custo em rela√ß√£o ao Mistral Large e NeMo.
* **Baixa lat√™ncia** ‚Üí Respostas mais r√°pidas em compara√ß√£o com modelos maiores da Mistral.
* **Flex√≠vel** ‚Üí Pode ser implantado em diferentes ambientes, exigindo menos recursos.

### **Casos de uso ideais** :

* **Tarefas baseadas em texto** , como sumariza√ß√£o, an√°lise de sentimento e tradu√ß√£o.
* **Aplica√ß√µes com chamadas frequentes** ‚Üí Devido ao custo reduzido.
* **Tarefas de c√≥digo de baixa lat√™ncia** , como revis√£o e sugest√µes de c√≥digo.

---

## **Comparando Mistral Small e Mistral Large**

Abaixo, h√° um teste para comparar a lat√™ncia entre **Mistral Small** e  **Mistral Large** .

A diferen√ßa no tempo de resposta pode variar entre  **3 a 5 segundos** , al√©m das diferen√ßas no estilo e tamanho da resposta.

```python
import os 
endpoint = "https://models.inference.ai.azure.com"
model_name = "Mistral-small"
token = os.environ["GITHUB_TOKEN"]

client = ChatCompletionsClient(
    endpoint=endpoint,
    credential=AzureKeyCredential(token),
)

response = client.complete(
    messages=[
        SystemMessage(content="Voc√™ √© um assistente √∫til para programa√ß√£o."),
        UserMessage(content="Voc√™ pode escrever uma fun√ß√£o em Python para o teste FizzBuzz?"),
    ],
    temperature=1.0,
    top_p=1.0,
    max_tokens=1000,
    model=model_name
)

print(response.choices[0].message.content)
```

O mesmo c√≥digo pode ser executado para  **Mistral Large** , alterando `model_name = "Mistral-large"`.

---

## **Mistral NeMo**

O **Mistral NeMo** √© um modelo de c√≥digo aberto com  **licen√ßa Apache2** , sendo considerado uma evolu√ß√£o do  **Mistral 7B** .

### **Principais caracter√≠sticas do Mistral NeMo** :

* **Tokeniza√ß√£o mais eficiente** ‚Üí Usa o **Tekken tokenizer** em vez do mais comum `tiktoken`, proporcionando melhor desempenho para m√∫ltiplos idiomas e c√≥digo.
* **Ajust√°vel por fine-tuning** ‚Üí O modelo base est√° dispon√≠vel para ajuste fino, permitindo maior flexibilidade em aplica√ß√µes personalizadas.
* **Suporte nativo a Function Calling** ‚Üí Assim como o Mistral Large, pode chamar fun√ß√µes externas.

---

### **Comparando Tokenizadores**

Este exemplo compara como o **Mistral NeMo** e o **Mistral Large** lidam com tokeniza√ß√£o.

O **NeMo retorna menos tokens** para o mesmo prompt, demonstrando sua efici√™ncia.

```bash
pip install mistral-common
```

```python
from mistral_common.protocol.instruct.messages import UserMessage
from mistral_common.protocol.instruct.request import ChatCompletionRequest
from mistral_common.tokens.tokenizers.mistral import MistralTokenizer

# Tokenizador Mistral NeMo
model_name = "open-mistral-nemo"
tokenizer = MistralTokenizer.from_model(model_name)

# Exemplo de tokeniza√ß√£o
tokenized = tokenizer.encode_chat_completion(
    ChatCompletionRequest(
        messages=[UserMessage(content="Como est√° o tempo hoje em Paris?")],
        model=model_name,
    )
)

tokens, text = tokenized.tokens, tokenized.text
print(len(tokens))  # Exibe a contagem de tokens
```

Para comparar com  **Mistral Large** , basta trocar `model_name = "mistral-large-latest"`.

---

## **Aprendizado cont√≠nuo: Continue sua jornada!**

Ap√≥s concluir esta li√ß√£o, explore mais sobre IA generativa na nossa [cole√ß√£o de aprendizado](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst).

---

### üöÄ  **Resumo** :

* **Mistral Large 2** ‚Üí Modelo poderoso para empresas, com grande janela de contexto e suporte a v√°rias l√≠nguas.
* **Mistral Small** ‚Üí Modelo compacto e eficiente, ideal para tarefas r√°pidas e custo reduzido.
* **Mistral NeMo** ‚Üí Modelo open-source, otimizado para tokeniza√ß√£o e  **fine-tuning** .

Se precisar de mais alguma informa√ß√£o ou explica√ß√£o sobre os conceitos, me avise! üöÄ
